# Deep Reinforcement Learning for Tarot (V4)

This module implements a **PPO (Proximal Policy Optimization)** agent to learn optimal Tarot strategy through self-play.

## Architecture

### Single-Agent Training
- **1 RL agent** (player_0) learns to play Tarot
- **3 opponent bots** (players 1-3) use fixed strategies (bot-naive, bot-random, or frozen RL snapshots)
- Agent learns both **taker** and **defense** roles through experience

### State Representation (506 dimensions)
| Feature Group | Dimensions | Description |
|---------------|------------|-------------|
| Hand | 78 | Multi-hot encoding of cards in hand |
| Legal Moves Mask | 78 | Binary mask (playable cards) |
| Current Trick | 312 | One-hot per position (4 players Ã— 78 cards) |
| Position in Trick | 4 | One-hot (1st/2nd/3rd/4th to play) |
| Trick Context | 27 | Trump led, suit, highest trump |
| Game Context | 7 | Taker status, contract type, oudlers, progress |

### Action Space
- **Discrete(78)**: Choose one card from hand
- **Masked actions**: Only legal moves are selectable
- **Automatic validation**: Invalid actions penalized

### Reward
- **Sparse mode** (default): +1 for winning contract (as taker or defense), 0 for losing
- **Dense mode**: Normalized score differential

---

## Quick Start

### 1. Test Environment

```bash
cd backend
uv run python scripts/test_env.py
```

This runs 3 full episodes to verify the environment works correctly.

### 2. Train an Agent

**Basic training** (8 parallel environments, 500K timesteps):
```bash
uv run python scripts/train_rl.py
```

**Custom training** (adjust hyperparameters):
```bash
uv run python scripts/train_rl.py \
    --opponent bot-naive \
    --timesteps 1000000 \
    --n-envs 16 \
    --learning-rate 3e-4
```

**Training options:**
- `--opponent`: Opponent strategy (`bot-naive`, `bot-random`, `rl-snapshot-v1`)
- `--timesteps`: Total training steps (default: 500,000)
- `--n-envs`: Parallel environments (default: 8)
- `--learning-rate`: PPO learning rate (default: 3e-4)
- `--reward-mode`: `sparse` (default) or `dense`
- `--resume`: Path to checkpoint to resume from
- `--no-multiprocessing`: Use single-process training (debugging)

**Outputs:**
- Model checkpoints: `models/checkpoints/ppo_tarot_*.zip`
- Final model: `models/ppo_tarot_v1.zip`
- Tensorboard logs: `runs/PPO_*/`

### 3. Monitor Training

```bash
tensorboard --logdir runs/
```

Open [http://localhost:6006](http://localhost:6006) to see:
- **Loss curves**: Policy loss, value loss, entropy
- **Rewards**: Episode rewards over time
- **Win rates**: Evaluation performance

### 4. Evaluate Trained Agent

**Evaluate vs bot-naive** (100 episodes):
```bash
uv run python scripts/evaluate_rl.py \
    --model models/ppo_tarot_v1.zip \
    --opponent bot-naive \
    --episodes 100
```

**Stochastic evaluation** (explore during testing):
```bash
uv run python scripts/evaluate_rl.py \
    --model models/ppo_tarot_v1.zip \
    --opponent bot-naive \
    --episodes 100 \
    --stochastic
```

**Evaluation output:**
```
============================================================
EVALUATION RESULTS
============================================================
Episodes: 100
Average Reward: 0.580
Average Episode Length: 18.0

Overall Win Rate: 58.0% (58/100)

Win Rate as Taker: 55.0% (22/40)
Win Rate as Defense: 60.0% (36/60)
============================================================
```

---

## File Structure

```
backend/
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ card_encoder.py          # One-hot encoding for 78 Tarot cards
â”‚   â”œâ”€â”€ state_encoder.py         # 506-dim state representation
â”‚   â”œâ”€â”€ tarot_env.py             # Gymnasium environment (SB3-compatible)
â”‚   â”œâ”€â”€ config.py                # Hyperparameters and configuration
â”‚   â””â”€â”€ models/                  # (empty - for future custom models)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_rl.py              # Training script with SB3 PPO
â”‚   â”œâ”€â”€ evaluate_rl.py           # Evaluation script
â”‚   â””â”€â”€ test_env.py              # Environment testing script
â””â”€â”€ models/
    â”œâ”€â”€ checkpoints/             # Training checkpoints (autogenerated)
    â””â”€â”€ ppo_tarot_v1.zip         # Final trained model (after training)
```

---

## Training Pipeline

### Phase 1: Baseline vs bot-naive (500K steps)
```bash
uv run python scripts/train_rl.py \
    --opponent bot-naive \
    --timesteps 500000 \
    --n-envs 8
```

**Expected result**: ~55-60% win rate vs bot-naive

### Phase 2: Curriculum Learning (vs stronger opponents)

**Option A: Train against frozen RL snapshot**
```bash
# Copy trained model to create frozen opponent
cp models/ppo_tarot_v1.zip models/frozen_snapshot_v1.zip

# Continue training vs frozen version
uv run python scripts/train_rl.py \
    --opponent rl-frozen-v1 \
    --timesteps 500000 \
    --resume models/checkpoints/ppo_tarot_500000_steps.zip
```

**Option B: Gradually increase difficulty**
```bash
# Stage 1: vs bot-random (easy)
uv run python scripts/train_rl.py --opponent bot-random --timesteps 200000

# Stage 2: vs bot-naive (medium)
uv run python scripts/train_rl.py --opponent bot-naive --timesteps 500000 --resume models/checkpoints/ppo_tarot_200000_steps.zip

# Stage 3: vs self (hard)
uv run python scripts/train_rl.py --opponent rl-frozen-v1 --timesteps 500000 --resume models/checkpoints/ppo_tarot_700000_steps.zip
```

---

## Hyperparameter Tuning

**Key hyperparameters** (in [config.py](rl/config.py)):

| Parameter | Default | Description |
|-----------|---------|-------------|
| `learning_rate` | 3e-4 | Adam learning rate |
| `n_steps` | 2048 | Steps per environment before PPO update |
| `batch_size` | 64 | Minibatch size for gradient descent |
| `n_epochs` | 10 | Gradient descent epochs per update |
| `gamma` | 0.99 | Discount factor for rewards |
| `gae_lambda` | 0.95 | GAE lambda for advantage estimation |
| `clip_range` | 0.2 | PPO clipping parameter |
| `ent_coef` | 0.01 | Entropy coefficient (exploration) |

**Tuning tips:**
- **Faster learning**: Increase `learning_rate` to `5e-4` or decrease `n_steps` to `1024`
- **More stable**: Decrease `learning_rate` to `1e-4` or increase `batch_size` to `128`
- **More exploration**: Increase `ent_coef` to `0.05`

---

## Troubleshooting

### Training is slow
- Increase `--n-envs` (more parallel environments)
- Reduce `n_steps` in config (more frequent updates)
- Use `--no-multiprocessing` to diagnose issues (single-threaded)

### Agent not learning
- Check Tensorboard: Is entropy decreasing? Is value loss converging?
- Try `--reward-mode dense` (more informative signal)
- Increase `ent_coef` (more exploration)
- Verify environment with `scripts/test_env.py`

### Out of memory
- Reduce `--n-envs` (fewer parallel environments)
- Reduce `n_steps` or `batch_size` in config

### Invalid action errors
- Check Tensorboard for `invalid_action` rate
- Should be 0% - if not, there's a bug in legal moves masking

---

## Next Steps (Phase 4: Transformer)

After MLP baseline achieves 55-60% win rate:

1. **Implement Transformer policy** in `rl/models/transformer_policy.py`
2. **Self-attention on hand cards** to model synergies (e.g., trump sequences, flushes)
3. **Custom SB3 policy** to integrate Transformer with PPO
4. **Target**: 70%+ win rate vs bot-naive

See [v4-deep-rl-implementation-plan.md](/home/terrand/.claude/plans/v4-deep-rl-implementation-plan.md) for details.

---

## Technical Details

### Why Stable-Baselines3?
- **Production-ready**: Battle-tested PPO implementation
- **Vectorized envs**: 8-16 games in parallel (3x faster training)
- **Monitoring**: Built-in Tensorboard logging
- **Callbacks**: Automatic checkpointing and evaluation

### Why PPO?
- **On-policy**: Learns from self-play trajectories
- **Stable**: Clipped objective prevents catastrophic updates
- **Sample efficient**: Reuses data via multiple epochs
- **State-of-the-art**: Used in OpenAI Five (Dota 2), AlphaStar (StarCraft)

### Single-Agent Design
Training 1 agent vs 3 fixed bots is simpler than 4-agent co-training:
- **Gym-compatible**: Standard RL environment
- **Stable training**: Opponent policies don't shift during training
- **Curriculum-friendly**: Gradually increase opponent difficulty
- **Efficient**: No need for multi-agent RL algorithms (MADDPG, etc.)

---

## Citation

**Implementation**: Based on [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3)
**Algorithm**: PPO ([Schulman et al. 2017](https://arxiv.org/abs/1707.06347))

**Generated with**: Claude Sonnet 4.5 ðŸ¤–
